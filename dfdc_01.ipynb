{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from PIL import Image\n",
    "import pathlib \n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total real images: 13233\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "lfw_root = 'C:/Users/Mahesh/OneDrive/Desktop/PyTorch/DATASETS/FaceForensics/real_images_lfw'\n",
    "\n",
    "real_image_paths = []\n",
    "\n",
    "for root, dirs, files in os.walk(lfw_root):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):\n",
    "            image_path = os.path.join(root, file)\n",
    "            real_image_paths.append(image_path)\n",
    "\n",
    "print(f\"Total real images: {len(real_image_paths)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total deepfake images: 20351\n"
     ]
    }
   ],
   "source": [
    "deepfake_root = 'C:/Users/Mahesh/OneDrive/Desktop/PyTorch/DATASETS/FaceForensics/deepfake_images'\n",
    "\n",
    "deepfake_image_paths = []\n",
    "\n",
    "for root, dirs, files in os.walk(deepfake_root):\n",
    "    for file in files:\n",
    "        if file.endswith(\".png\"): \n",
    "            image_path = os.path.join(root, file)\n",
    "            deepfake_image_paths.append(image_path)\n",
    "\n",
    "print(f\"Total deepfake images: {len(deepfake_image_paths)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 33584\n",
      "Total labels: 33584\n"
     ]
    }
   ],
   "source": [
    "all_image_paths = real_image_paths + deepfake_image_paths\n",
    "\n",
    "labels = [0] * len(real_image_paths) + [1] * len(deepfake_image_paths)\n",
    "\n",
    "print(f\"Total images: {len(all_image_paths)}\")\n",
    "print(f\"Total labels: {len(labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_image_paths, test_image_paths, train_labels, test_labels = train_test_split(\n",
    "    all_image_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(image_path).convert('RGB') \n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)), \n",
    "    transforms.ToTensor(),  \n",
    "])\n",
    "\n",
    "train_dataset = DeepfakeDataset(train_image_paths, train_labels, transform=transform)\n",
    "test_dataset = DeepfakeDataset(test_image_paths, test_labels, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size : 26867 samples.\n",
      "Testing Size  : 6717 samples.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Size : {len(train_image_paths)} samples.\")\n",
    "print(f\"Testing Size  : {len(test_image_paths)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mahesh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=True, img_size = 32)\n",
    "\n",
    "num_features = model.head.in_features\n",
    "model.head = nn.Linear(num_features, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mahesh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\timm\\models\\vision_transformer.py:92: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  x = F.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.0347, Train Accuracy: 98.62%\n",
      "Epoch [1/10], Test Loss: 0.0167, Test Accuracy: 99.48%\n",
      "Epoch [2/10], Train Loss: 0.0066, Train Accuracy: 99.83%\n",
      "Epoch [2/10], Test Loss: 0.0065, Test Accuracy: 99.87%\n",
      "Epoch [3/10], Train Loss: 0.0040, Train Accuracy: 99.90%\n",
      "Epoch [3/10], Test Loss: 0.0009, Test Accuracy: 99.97%\n",
      "Epoch [4/10], Train Loss: 0.0077, Train Accuracy: 99.79%\n",
      "Epoch [4/10], Test Loss: 0.0004, Test Accuracy: 99.97%\n",
      "Epoch [5/10], Train Loss: 0.0024, Train Accuracy: 99.94%\n",
      "Epoch [5/10], Test Loss: 0.0098, Test Accuracy: 99.70%\n",
      "Epoch [6/10], Train Loss: 0.0187, Train Accuracy: 99.55%\n",
      "Epoch [6/10], Test Loss: 0.0018, Test Accuracy: 99.94%\n",
      "Epoch [7/10], Train Loss: 0.0046, Train Accuracy: 99.90%\n",
      "Epoch [7/10], Test Loss: 0.0047, Test Accuracy: 99.93%\n",
      "Epoch [8/10], Train Loss: 0.0043, Train Accuracy: 99.89%\n",
      "Epoch [8/10], Test Loss: 0.0053, Test Accuracy: 99.85%\n",
      "Epoch [9/10], Train Loss: 0.0102, Train Accuracy: 99.68%\n",
      "Epoch [9/10], Test Loss: 0.0306, Test Accuracy: 99.21%\n",
      "Epoch [10/10], Train Loss: 0.0069, Train Accuracy: 99.82%\n",
      "Epoch [10/10], Test Loss: 0.0029, Test Accuracy: 99.91%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10 \n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).squeeze()\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = correct / total\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_acc)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_acc*100:.2f}%\")\n",
    "\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device).float()\n",
    "\n",
    "            outputs = model(images).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(test_loader)\n",
    "    epoch_acc = correct / total\n",
    "    test_losses.append(epoch_loss)\n",
    "    test_accuracies.append(epoch_acc)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Test Loss: {epoch_loss:.4f}, Test Accuracy: {epoch_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def predict(image):\n",
    "    image = transform(image).unsqueeze(0).to(device)  \n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        probability = torch.sigmoid(output).item()\n",
    "\n",
    "    if probability > 0.5:\n",
    "        return \"Deepfake\", probability\n",
    "    else:\n",
    "        return \"Real\", probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=gr.Image(type=\"pil\"),\n",
    "    outputs=[gr.Label(num_top_classes=1), gr.Number(label=\"Confidence Score\")],\n",
    "    title=\"Deepfake Recognition\",\n",
    "    description=\"Upload an image to check if it is real or deepfake.\"\n",
    ")\n",
    "interface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
